{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download_Python\\lib\\importlib\\__init__.py:127: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Thực thi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load folder .docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_data_docx = r\"data\\trich_dan_luat\\docx\\Luat\"\n",
    "# os.chdir(folder_data_docx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load file .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name_json = r'D:\\Download_Github_Desktop\\Law_ChatBot\\Vector_Database_Qdrant\\data\\trich_dan_luat\\json\\extract_Data_FileName.json'\n",
    "# with open(file_name_json, 'r', encoding='utf-8') as file:\n",
    "#     data_json = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Model Embedidng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Download_Python\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_MODEL = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDINGS_MODEL,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = SemanticChunker(\n",
    "#     embeddings=embeddings, breakpoint_threshold_type=\"gradient\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# URL_QDRANT_3 = os.getenv(\"URL_QDRANT_3\")\n",
    "# API_QDRANT_3 = os.getenv(\"API_QDRANT_3\")\n",
    "\n",
    "# url = URL_QDRANT_3\n",
    "# api_key = API_QDRANT_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, f in enumerate(glob.glob('*.docx')):\n",
    "# \traw_documents = Docx2txtLoader(f).load()\n",
    "# \tdocs = text_splitter.split_documents(raw_documents)\n",
    "# \tmetadata = data_json[idx]\n",
    "# \tfor doc in docs:\n",
    "# \t\tsetattr(doc, \"metadata\", metadata) \n",
    "# \tqdrant = Qdrant.from_documents(\n",
    "# \t\tdocuments=docs,\n",
    "# \t\tembedding=embeddings,\n",
    "# \t\turl=url,\n",
    "# \t\tapi_key = api_key,\n",
    "# \t\tcollection_name=\"semantic_Luat_bkai\",\n",
    "# \t\tmetadata_payload_key=\"metadata\",\n",
    "# \t\tprefer_grpc=True\n",
    "# \t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải dữ liệu từ file JSON\n",
    "with open(r'data\\trich_dan_luat\\json\\demo_Article-Section_Combined_Output_More_Keywords.json', 'r', encoding='utf-8') as f:\n",
    "    data_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = []\n",
    "for data in data_list:\n",
    "    # Lấy page_content từ key \"combine_Article_Content\"\n",
    "    page_content = data.get(\"combine_Article_Content\", \"\")\n",
    "    \n",
    "    # Tạo metadata từ các key khác\n",
    "    metadata = {key: value for key, value in data.items() #if key != \"combine_Article_Content\"\n",
    "                }\n",
    "    \n",
    "    # Tạo Document và thêm vào danh sách documents\n",
    "    doc = Document(page_content=page_content, metadata=metadata)\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    documents = documents,\n",
    "    embedding = embeddings,\n",
    "    url = \"http://localhost:6333/\",\n",
    "    # api_key = api_key,\n",
    "    # prefer_grpc=True,\n",
    "    collection_name = \"Luat_bkai_Article-Section_More_Keywords\",\n",
    "\tmetadata_payload_key=\"metadata\",\n",
    "    timeout = 300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
