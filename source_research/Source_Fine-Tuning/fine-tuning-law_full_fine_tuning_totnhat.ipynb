{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9530536,"sourceType":"datasetVersion","datasetId":5803988},{"sourceId":9575846,"sourceType":"datasetVersion","datasetId":5837643},{"sourceId":9612091,"sourceType":"datasetVersion","datasetId":5865266},{"sourceId":9638094,"sourceType":"datasetVersion","datasetId":5884991},{"sourceId":9699692,"sourceType":"datasetVersion","datasetId":5931276},{"sourceId":9700754,"sourceType":"datasetVersion","datasetId":5932098},{"sourceId":9708070,"sourceType":"datasetVersion","datasetId":5937674},{"sourceId":9812441,"sourceType":"datasetVersion","datasetId":6015536}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1.Import Libary","metadata":{}},{"cell_type":"code","source":"! pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-11-05T11:07:03.790282Z","iopub.execute_input":"2024-11-05T11:07:03.790615Z","iopub.status.idle":"2024-11-05T11:07:16.702250Z","shell.execute_reply.started":"2024-11-05T11:07:03.790579Z","shell.execute_reply":"2024-11-05T11:07:16.701132Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments,Trainer, EarlyStoppingCallback\nimport numpy as np\nimport collections\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset, DatasetDict\nimport evaluate\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:16.704074Z","iopub.execute_input":"2024-11-05T11:07:16.704382Z","iopub.status.idle":"2024-11-05T11:07:39.837386Z","shell.execute_reply.started":"2024-11-05T11:07:16.704350Z","shell.execute_reply":"2024-11-05T11:07:39.836623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.Data Preprocessing","metadata":{}},{"cell_type":"code","source":"\n\ndef read_dataset(file_path):\n    \"\"\"Đọc dataset từ file CSV.\"\"\"\n    df = pd.read_csv(file_path)\n    df['context'] = df['context'].astype(str)\n    df['question'] = df['question'].astype(str)\n    df['answer'] = df['answer'].astype(str)\n    return df\n\ndef find_start_index(context, answer):\n    \"\"\"Tìm chỉ số bắt đầu của answer trong context.\"\"\"\n    return str(context).find(str(answer))\n\ndef prepare_dataset(df):\n    TRAIN_RATIO = 0.8\n    VAL_RATIO = 0.1\n    \"\"\"Chuẩn bị dataset cho huấn luyện, xác thực và kiểm tra.\"\"\"\n    df['start_index'] = df.apply(lambda row: find_start_index(context=row['context'], answer=row['answer']), axis=1)\n    df = df[df['start_index'] != -1]\n\n    dataset_temp = []\n    for _, row in df.iterrows():\n        sample = {\n            'context': row['context'],\n            'question': row['question'],\n            'answer': {'text': [row['answer']], 'answer_start': [row['start_index']]}\n        }\n        dataset_temp.append(sample)\n\n    dataset = pd.DataFrame(dataset_temp)\n\n    num_of_total_sample = len(dataset)\n    num_of_train_sample = TRAIN_RATIO * num_of_total_sample\n    num_of_val_sample = VAL_RATIO * num_of_total_sample\n\n    train_set = dataset.sample(n=int(num_of_train_sample), random_state=42)\n    dataset.drop(index=train_set.index, inplace=True)\n\n    val_set = dataset.sample(n=int(num_of_val_sample), random_state=42)\n    dataset.drop(index=val_set.index, inplace=True)\n\n    return Dataset.from_pandas(train_set), Dataset.from_pandas(val_set), Dataset.from_pandas(dataset)\n\ndef preprocess_training_validation_examples(examples, tokenizer, max_length, stride):\n    \"\"\"Tiền xử lý ví dụ huấn luyện cho mô hình.\"\"\"\n    inputs = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answer\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs\n\ndef preprocess_test_examples(examples, tokenizer, max_length, stride):\n    \"\"\"Tiền xử lý ví dụ cho bộ kiểm tra và xác thực.\"\"\"\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"question\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:39.838681Z","iopub.execute_input":"2024-11-05T11:07:39.838983Z","iopub.status.idle":"2024-11-05T11:07:39.861413Z","shell.execute_reply.started":"2024-11-05T11:07:39.838951Z","shell.execute_reply":"2024-11-05T11:07:39.860374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.Fine-Tuning Model","metadata":{}},{"cell_type":"markdown","source":"> Function Evaluation","metadata":{}},{"cell_type":"code","source":"\nmetric = evaluate.load(\"squad\")\n\ndef compute_metrics(start_logits, end_logits, features, examples, n_best, max_answer_length, metric):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"question\"]\n        context = example[\"context\"]\n        answers = []\n\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"question\"], \"answers\": ex[\"answer\"]} for ex in examples]\n    \n    for index, (i, ii) in enumerate(zip(predicted_answers, theoretical_answers)):\n        if index >= 2:  # Dừng sau 2 mẫu\n            break\n        print(\"-\" * 99)\n        print(f\"ID: {i['id']}\")\n        print(f\"Predicted Answer: {i['prediction_text']}\")\n        print(f\"Correct Answers: {', '.join(ii['answers']['text'])}\")\n        print(\"-\" * 99)\n    \n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:39.864445Z","iopub.execute_input":"2024-11-05T11:07:39.864888Z","iopub.status.idle":"2024-11-05T11:07:41.942975Z","shell.execute_reply.started":"2024-11-05T11:07:39.864831Z","shell.execute_reply":"2024-11-05T11:07:41.942025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Function Load Model and Load tokenizer","metadata":{}},{"cell_type":"code","source":"def load_model_and_tokenizer(model_checkpoint):\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n    model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:41.944443Z","iopub.execute_input":"2024-11-05T11:07:41.944911Z","iopub.status.idle":"2024-11-05T11:07:41.950046Z","shell.execute_reply.started":"2024-11-05T11:07:41.944866Z","shell.execute_reply":"2024-11-05T11:07:41.948952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> declare hyperparameters","metadata":{}},{"cell_type":"code","source":"wandb.login(key = '8a5cbfdaa29778a896996cc679358b1d96cf66b0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:41.951191Z","iopub.execute_input":"2024-11-05T11:07:41.951479Z","iopub.status.idle":"2024-11-05T11:07:44.210151Z","shell.execute_reply.started":"2024-11-05T11:07:41.951449Z","shell.execute_reply":"2024-11-05T11:07:44.209224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_CHECKPOINT = \"google-bert/bert-base-multilingual-cased\"\nMAX_LENGTH = 512\nSTRIDE = 380\nN_BEST = 180\nMAX_ANSWER_LENGTH = 2000\n\n # Early stopping callback\nearly_stopping_callback = EarlyStoppingCallback(early_stopping_patience=4 ,early_stopping_threshold=0.001)\n\n    # Training arguments\ntraining_args = TrainingArguments(\n        output_dir=\"./bert_question_answer\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_strategy=\"epoch\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=32,\n        gradient_accumulation_steps=6,\n        num_train_epochs=15,\n        disable_tqdm=False,\n        weight_decay=0.15,\n        save_total_limit=3,\n        optim=\"adamw_hf\",\n        fp16=True,\n        max_grad_norm=0.3,\n        warmup_ratio=0.1,\n        group_by_length=True,\n        report_to=\"wandb\",\n        load_best_model_at_end=True,\n        label_names=['start_positions', 'end_positions'],\n        lr_scheduler_type=\"linear\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:44.211224Z","iopub.execute_input":"2024-11-05T11:07:44.211869Z","iopub.status.idle":"2024-11-05T11:07:44.325030Z","shell.execute_reply.started":"2024-11-05T11:07:44.211834Z","shell.execute_reply":"2024-11-05T11:07:44.324125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Call functions read dataset","metadata":{}},{"cell_type":"code","source":"df = read_dataset('/kaggle/input/data-final/final_train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:44.326164Z","iopub.execute_input":"2024-11-05T11:07:44.326509Z","iopub.status.idle":"2024-11-05T11:07:46.032884Z","shell.execute_reply.started":"2024-11-05T11:07:44.326475Z","shell.execute_reply":"2024-11-05T11:07:46.031882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:46.034083Z","iopub.execute_input":"2024-11-05T11:07:46.034402Z","iopub.status.idle":"2024-11-05T11:07:46.041301Z","shell.execute_reply.started":"2024-11-05T11:07:46.034370Z","shell.execute_reply":"2024-11-05T11:07:46.040373Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> call function  prepare_dateset","metadata":{}},{"cell_type":"code","source":"train_set, val_set, test_set = prepare_dataset(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:46.044785Z","iopub.execute_input":"2024-11-05T11:07:46.045177Z","iopub.status.idle":"2024-11-05T11:07:48.130164Z","shell.execute_reply.started":"2024-11-05T11:07:46.045145Z","shell.execute_reply":"2024-11-05T11:07:48.129170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f'tập train {len(train_set)}')\nprint(f'tập val {len(val_set)}')\nprint(f'test_test{len(test_set)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:48.131524Z","iopub.execute_input":"2024-11-05T11:07:48.131920Z","iopub.status.idle":"2024-11-05T11:07:48.137276Z","shell.execute_reply.started":"2024-11-05T11:07:48.131876Z","shell.execute_reply":"2024-11-05T11:07:48.136382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> call function Load model and tokenizer","metadata":{}},{"cell_type":"code","source":"model, tokenizer = load_model_and_tokenizer(MODEL_CHECKPOINT)\nmodel.to('cuda')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:48.138676Z","iopub.execute_input":"2024-11-05T11:07:48.139053Z","iopub.status.idle":"2024-11-05T11:07:56.482314Z","shell.execute_reply.started":"2024-11-05T11:07:48.139000Z","shell.execute_reply":"2024-11-05T11:07:56.481399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Embedding ","metadata":{}},{"cell_type":"code","source":"\ntrain_dataset = train_set.map(\n        lambda examples: preprocess_training_validation_examples(examples, tokenizer, MAX_LENGTH, STRIDE),\n        batched=True,\n        remove_columns=train_set.column_names,\n    )\nval_dataset = val_set.map(\n        lambda examples: preprocess_training_validation_examples(examples, tokenizer, MAX_LENGTH, STRIDE),\n        batched=True,\n        remove_columns=train_set.column_names,\n    )\ntest_dataset = test_set.map(\n        lambda examples: preprocess_test_examples(examples, tokenizer, MAX_LENGTH, STRIDE),\n        batched=True,\n        remove_columns=train_set.column_names,\n    )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:07:56.483458Z","iopub.execute_input":"2024-11-05T11:07:56.483762Z","iopub.status.idle":"2024-11-05T11:08:21.908523Z","shell.execute_reply.started":"2024-11-05T11:07:56.483729Z","shell.execute_reply":"2024-11-05T11:08:21.907595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> training process","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset ,\n        callbacks=[early_stopping_callback],\n    )\ntrainer.can_return_loss = True\ntrainer.train()\n    \ntrainer.save_model(r\"./bert_question_answer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:08:21.909665Z","iopub.execute_input":"2024-11-05T11:08:21.910464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Evaluation","metadata":{}},{"cell_type":"code","source":"predictions, _, _ = trainer.predict(test_dataset)\nstart_logits, end_logits = predictions    \nresults = compute_metrics(start_logits, end_logits, test_dataset, test_set, N_BEST, MAX_ANSWER_LENGTH,metric)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}